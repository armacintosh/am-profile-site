{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI Consultant & Scientist","text":""},{"location":"#ai-consultant-scientist","title":"AI Consultant &amp; Scientist","text":""},{"location":"#grounding-ai-in-science-transparency","title":"Grounding AI in Science &amp; Transparency","text":"<p>Bridging the \"Trust Gap\" between technical execution and scientific integrity.</p> <ul> <li> <p>Scientific Validation: Move from \"it works in the lab\" to market-ready evidence.</p> </li> <li> <p>Explainable AI (XAI): Turn black-box models into transparent, trustworthy systems.</p> </li> <li> <p>Risk &amp; Ethics: Align with national standards (CAN/DGSI 101) and global best practices.</p> </li> <li> <p>Funding Success: Secure R&amp;D grants with valid technical and scientific proposals.</p> </li> </ul> <p>Start the Conversation </p>"},{"location":"#about-me","title":"About Me","text":"<p>I am an AI Consultant &amp; Scientist helping Digital Health companies build trust through rigorous validation and Explainable AI (XAI). With a PhD in Biomedical Engineering and a focus on \"AI for Good,\" I help teams prove that their innovations are reliable, fair, and effective.</p> <p>Whether you need a peer-reviewed validation study, a deep-dive data analysis, or a strategy to secure non-dilutive funding, I provide the scientific proof and technical execution required to move your product from a prototype to a category leader.</p>"},{"location":"#key-value-opportunities","title":"Key Value Opportunities","text":"<ul> <li> <p> The \"Scientist in the Room\"</p> <p>Provide the professional authority (PhD/Published Author) needed to validate products that require evidence but sit outside traditional clinical trial structures.</p> </li> <li> <p> Software-First Validation</p> <p>Specializing in the unique speed and iteration of Digital Health, distinct from the slow-moving world of pharma and hardware.</p> </li> <li> <p> Defensible AI</p> <p>Translating complex AI ethics (CAN/DGSI 101) into actionable product features that serve as a competitive advantage against less-validated competitors.</p> </li> <li> <p> Evidence-Based Growth</p> <p>Secure trust, increase valuation, and win over investors by backing your claims with real data and rigorous analysis.</p> </li> </ul> <ul> <li> <p> Let's connect!</p> <p>Ready to create defensible, transparent AI? Schedule a free strategy session to discuss your validation roadmap.</p> <p>Start the Conversation </p> </li> </ul>"},{"location":"about/","title":"Alex MacIntosh, PhD","text":"<p>AI Consultant &amp; Scientist </p>"},{"location":"about/#executive-summary-bridging-the-trust-gap","title":"Executive Summary: Bridging the \"Trust Gap\"","text":"<p>I am an AI Consultant &amp; Scientist, helping Digital Health companies build technology that is transparent, explainable, and backed by real evidence. I focus on making sure your AI is used in a \"good\" way\u2014meaning it follows best practices and provides results that users and investors can actually trust. With a PhD in Biomedical Engineering and over 40 peer-reviewed research publications, I provide the scientific proof and technical execution your product needs to move from a lab prototype to a market leader.</p>"},{"location":"about/#professional-experience","title":"Professional Experience","text":""},{"location":"about/#acuity-insights-scientist-senior-manager-research","title":"Acuity Insights | Scientist &amp; Senior Manager, Research","text":"<p>2020 \u2013 Present</p> <ul> <li>R&amp;D: Led the prototyping and evaluation of NLP and LLM algorithms, facilitating the transition from manual expert rating to automated, evidence-backed validation systems.</li> <li>Revenue Generation: Conceptualized and launched a Research Services portfolio that generated $300,000 in new market revenue and 130 qualified leads in its first year.</li> <li>Client Consulting: Designed 60+ analytical and quality assurance reports evaluating the validity, fairness, and equity of high-stakes assessment processes.</li> <li>Strategic Leadership: Scaled the research department from 2 to 12 members, aligning R&amp;D with company-level Key Results (KRs) and securing $100K in annual internal grant funding.</li> </ul>"},{"location":"about/#bloorview-research-institute-researcher","title":"Bloorview Research Institute | Researcher","text":"<p>2014 \u2013 2019</p> <ul> <li>Personalized Rehabilitation AI: Designed and evaluated ability-based balancing algorithms for therapeutic exergames, leveraging real-time biofeedback to drive measurable improvements in physical recovery outcomes.</li> <li>Global Research Strategy: Executed a joint-PhD between the University of Toronto and Universit\u00e9 Paris-Saclay, bridging Computer Science, Motor Control, and Biomedical Engineering.</li> </ul>"},{"location":"about/#gtd-scientific-inc-research-engineer-consultant","title":"GTD Scientific Inc. | Research Engineer (Consultant)","text":"<p>2014 \u2013 2016</p> <ul> <li>Predictive Modeling: Developed biomechanical simulations to support corporate decision-making and product evaluation.</li> <li>Evidence Synthesis: Authored technical manuscripts translating complex research findings into actionable insights for product development and procurement.</li> </ul>"},{"location":"about/#academic-appointments-education","title":"Academic Appointments &amp; Education","text":"<ul> <li> <p>Adjunct Faculty | Applied Modelling and Quantitative Methods | 2023 \u2013 Present   Trent University</p> </li> <li> <p>PhD, Biomedical Engineering &amp; Motor Control | 2016 \u2013 2019   University of Toronto &amp; Universit\u00e9 Paris-Saclay (Dual Degree) Top 6% nationally (CIHR Doctoral Research Award Evaluation)</p> </li> </ul>"},{"location":"about/#analysis-tech-stack","title":"Analysis &amp; Tech Stack","text":"<ul> <li>Languages &amp; Libraries: Python (Pandas, Polars, SciPy, TensorFlow, PyTorch, LangChain, LangGraph, GraphRAG, spaCy, FastAPI, Streamlit, Plotly, Weights &amp; Biases); R, SQL (PostgreSQL).</li> <li>Platforms &amp; Tools: Vertex AI, Gemini File Search, Docker, Firebase, Pinecone, Looker, and Qlik.</li> <li>Methodologies: Bayesian &amp; Classical Statistics, Mixed-Methods Studies, Psychometrics, Meta-analysis, Biomechanical Modelling, and User-Centered Design.</li> </ul>"},{"location":"about/#scientific-leadership-impact","title":"Scientific Leadership &amp; Impact","text":"<ul> <li>Funding: Secured over $500,000 in research funding through agencies including CIHR, Mitacs, and the Eiffel Excellence Scholarship (France).</li> <li>AI Policy: Technical Committee Member | Digital Governance Standards (CAN/DGSI 101). Contributing to national policy on the Ethical Use of AI by small and medium organizations.</li> </ul>"},{"location":"about/#patent-contributions","title":"Patent Contributions","text":"<ol> <li>Computer System and Method for Cognitive Tests for Scalable Precision Education (Acuity Insights, 2024): Interactive digital platforms for measuring and scoring cognitive performance through behavioral data.</li> <li>Natural Language Processing for Quality Assurance for Constructed-Response Tests (Altus Assessments, 2022): Novel architectures for automating the audit and validation of high-stakes textual assessments.</li> <li>Ability-based balancing for Exercise Video Games (Bloorview Research Institute, 2016): Dynamic, ability-based balancing algorithms that utilize real-time biofeedback to tailor rehabilitation experiences.</li> </ol>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2026/01/20/giving-feedback-at-scale-our-journey-into-fine-tuning-gpt-for-education/","title":"Giving Feedback at Scale: Our Journey into Fine-Tuning GPT for Education","text":"<p>We have all felt that moment of frustration: you put hours of thought into a complex response, only to receive a generic \"Good job\" or a cold numerical score. As a researcher at Acuity Insights, I have spent years looking at how we can make educational assessment more human, even when we are dealing with thousands of students.</p> <p>My team and I recently presented our findings on this challenge at The 40th ACM/SIGAPP Symposium on Applied Computing (SAC '25) in Catania, Italy. We wanted to know: Can we actually teach an AI to give feedback that feels personal, supportive, and\u2014most importantly\u2014useful?</p> <p>DOI: 10.1145/3672608.3707735</p>"},{"location":"blog/2026/01/20/giving-feedback-at-scale-our-journey-into-fine-tuning-gpt-for-education/#the-casper-challenge","title":"The \"Casper\" Challenge","text":"<p>We decided to test this on Casper, a high-stakes situational judgment test (SJT) that assesses social intelligence and professionalism. Unlike a math quiz, there are no strictly right or wrong answers in Casper; instead, applicants describe how they would handle complex ethical dilemmas or social challenges.</p> <p>Because these responses are so unique and diverse, you cannot just use a pre-written template for feedback. You need a system that can truly \"understand\" the nuances of a student\u2019s reasoning.</p>"},{"location":"blog/2026/01/20/giving-feedback-at-scale-our-journey-into-fine-tuning-gpt-for-education/#moving-beyond-simple-chatbots","title":"Moving Beyond Simple Chatbots","text":"<p>Most research to date has focused on using standard \"off-the-shelf\" AI models with basic prompting. But for high-stakes education, we needed more precision.</p> <p>We took GPT-3.5-Turbo and applied Parameter-Efficient Fine-Tuning (PEFT). Specifically, we used Low-Rank Adaptation (LoRA) to update the model\u2019s parameters specifically for the task of generating feedback.</p> <p>The most surprising part? We didn\u2019t need a massive amount of data. We developed a high-quality training set of just 124 hand-crafted feedback examples. We also integrated Chain-of-Thought (CoT) prompting, requiring the model to \"think step-by-step\" and explain its reasoning before writing the final message to the student.</p>"},{"location":"blog/2026/01/20/giving-feedback-at-scale-our-journey-into-fine-tuning-gpt-for-education/#what-we-discovered","title":"What We Discovered","text":"<p>To see if our fine-tuned tutor actually worked, we had independent judges, test experts, and real students evaluate the results.</p> <ul> <li>Structure &amp; Tone: 72.9% of the generated messages met every single structural criterion for effective feedback.</li> <li>Personalization: Over 94.9% of the messages were successfully tailored to the specific elements of the applicant's response.</li> <li>Actionability: Every single message\u2014except for those responding to perfect scores\u2014included concrete suggestions for improvement.</li> <li>User Satisfaction: In our survey of 164 participants, 84.8% expressed satisfaction with the feedback they received.</li> <li>Supportive Environment: Over 70% of respondents agreed the feedback was clear, logically organized, and supportive.</li> </ul>"},{"location":"blog/2026/01/20/giving-feedback-at-scale-our-journey-into-fine-tuning-gpt-for-education/#the-path-forward","title":"The Path Forward","text":"<p>Is it perfect? Not yet. Some users wanted even more detail, and we observed occasional linguistic slips. There is also a delicate balance to strike: we want to give students helpful tips without compromising the security of a high-stakes test.</p> <p>However, this study shows that we are on the right track. By combining expert human knowledge with specialized AI training, we can provide the kind of timely, personalized support that every learner deserves\u2014at a scale that was previously impossible.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/","title":"From Rehab Robotics to GraphRAG: Why Context is King","text":"<p>Before I was designing Generative AI architectures, I was building explainable machine learning applications for healthcare.</p> <p>Specifically, I worked on systems to help Occupational Therapists guide young people with Cerebral Palsy through home-based therapy. The challenge wasn't just \"detecting a movement.\" It was distinguishing between a therapeutic gesture and the \"noisy\" neurological commands often present in CP, like spasticity or muscle synergies.</p> <p>To make that work, we couldn't just throw raw data at a black box. We had to build strict calibration procedures to personalize the system to the individual\u2019s physiology. We had to select interpretable features\u2014like movement variability\u2014that gave therapists actual clinical insight rather than just a binary \"pass/fail\".</p> <p>I carried this obsession with context and calibration into my recent work with Large Language Models.</p> <p>Without strict grounding in human context, AI is just guessing.</p> <p>Whether you are fine-tuning GPT-3.5 to provide personalized feedback (which I explored in a recent paper) or building a documentation assistant, the core problem remains the same.</p> <p>Here is how I applied those lessons to a recent project: The Urban Education Explorer.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#the-challenge-the-api-documentation-swamp","title":"The Challenge: The API Documentation Swamp","text":"<p>I recently tackled a massive repository of educational data from the Urban Institute. We\u2019re talking about 3,000+ variables and 40+ endpoints\u2014everything from student outcomes to university tax records.</p> <p>The problem? The API is incredibly powerful, but accessing it requires navigating thousands of obscure variable codes like <code>cc_basic_2021</code>.</p> <p>The Problem</p> <p>For a developer or researcher trying to use this API, finding the right parameter is a nightmare. A standard \"out-of-the-box\" chatbot has no idea what <code>cc_basic_2021</code> means. If you ask it how to find \"admission rates,\" it will likely hallucinate a variable name that sounds plausible but doesn't exist in the documentation.</p> <p>I needed a way to \"calibrate\" the AI to the API documentation, ensuring it could guide users to the authoritative variable codes they actually needed.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#the-solution-client-side-graphrag-as-a-documentation-navigator","title":"The Solution: Client-Side GraphRAG as a Documentation Navigator","text":"<p>I built a custom, lightweight GraphRAG (Graph Retrieval-Augmented Generation) architecture.</p> <p>Crucially, this system does not analyze the university data itself. Instead, it analyzes the metadata\u2014the descriptions, formats, and relationships of the 3,000+ variable codes. It acts as an intelligent layer on top of the technical documentation.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#architecture-breakdown","title":"Architecture Breakdown","text":"<pre><code>flowchart LR\n    User[User Query] --&gt; Embed[Intent Understanding]\n    Embed -- \"Vector Search\" --&gt; VectorStore[(\"Vector Store\\n(Descriptions)\")]\n    VectorStore -- \"Top Matches\" --&gt; Graph[Contextualization\\n(Inference Graph)]\n    Graph -- \"Enriched Context\" --&gt; LLM[Gemini Flash Lite]\n    LLM --&gt; Response[Final Answer]\n\n    subgraph \"Client-Side Processing\"\n    Graph\n    end\n\n    subgraph \"Backend / Proxy\"\n    Embed\n    LLM\n    end</code></pre>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#1-intent-understanding-the-embedding","title":"1. Intent Understanding (The Embedding)","text":"<p>Just like we had to capture the \"intent\" of a hand movement despite muscle tremors, we first need to capture the intent of the user's query. When a user asks, \"What variable do I use for diversity stats?\", a Netlify Function proxies that text to Google\u2019s Gemini Embedding Model (text-embedding-004). This converts the fuzzy human language into a precise mathematical vector.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#2-the-search-client-side-vector-retrieval","title":"2. The Search (Client-Side Vector Retrieval)","text":"<p>We don't send that vector to the LLM yet. Instead, the application runs a Cosine Similarity search right in the browser, checking against a pre-computed vector store (<code>node_embeddings.json</code>) of variable descriptions. This \"Grounding\" step ensures the system is locked onto the official documentation before it ever tries to generate an answer.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#3-contextualization-the-inference-graph","title":"3. Contextualization (The Inference \"Graph\")","text":"<p>This is the most critical step. In my rehab work, we used \"Context Expansion\" to map a muscle signal to a clinical outcome. Here, I built a Rule-Based Inference Engine that acts as a Knowledge Graph.</p> <p>Instead of just retrieving a code like <code>cc_basic_2021</code>, the system procedurally generates its context by parsing the code structure:</p> <ul> <li>Where does this variable live? (Inferred via dataset relationship tags)</li> <li>What is the data format? (Inferred via nomenclature patterns, e.g., 'pct' -&gt; Percentage)</li> </ul> <p>This creates a \"virtual graph\" of relationships without needing a heavy graph database.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#4-synthesis-the-response","title":"4. Synthesis (The Response)","text":"<p>Finally, we package the original question + the retrieved variable codes and their inferred context, and send them to Gemini Flash Lite (gemini-2.5-flash-lite) via our Netlify proxy. Because we have \"calibrated\" the prompt with the exact API parameters required, the model doesn't hallucinate. It accurately tells the user: \"You are looking for <code>cc_basic_2021</code>, which represents the Carnegie Classification.\"</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#the-result-democratized-access","title":"The Result: Democratized Access","text":"<p>The Urban Education Explorer transforms a dense technical documentation site into a conversational interface.</p> <p>Key Outcome</p> <p>It allows users to ask high-level questions in plain English and instantly receive the precise, authoritative codes they need to query the database. It successfully disambiguates between thousands of similar variable names, ensuring that researchers are building their analysis on the correct underlying data fields.</p>"},{"location":"blog/2026/01/25/from-rehab-robotics-to-graphrag-why-context-is-king/#why-this-matters","title":"Why This Matters","text":"<p>In the rush to adopt Generative AI, it is easy to forget that these models are statistical predictors, not truth machines.</p> <p>My experience in pre-GenAI machine learning taught me that interpretable features and human context are not optional add-ons; they are the foundation of a reliable system.</p> <p>Whether interpreting the muscle signals of a child with CP or navigating the schema of a massive government database, the goal is the same: Use the AI to translate the user's intent, but use your architecture to ground the AI in reality.</p> <p>If you are struggling to make your complex API documentation accessible, let's connect. I help organizations build AI systems that actually understand their data structure.</p>"},{"location":"portfolio/","title":"Scientific Patents &amp; Projects","text":"<p>Selected contributions to AI, education, and rehabilitation technology.</p> <ul> <li> <p> Knowledge Graph on URBAN Data API Docs</p> <p></p> <p>Intelligent web application bridging the gap between complex data API and human curiosity using GraphRAG.</p> </li> <li> <p> NLP for Quality Assurance</p> <p>Automating the audit and validation of high-stakes textual assessments using Natural Language Processing.</p> </li> <li> <p> Ability-based Balancing for Exergames</p> <p>Dynamic balancing algorithms utilizing biofeedback for therapeutic video games.</p> </li> </ul>"},{"location":"portfolio/projects/project-1/","title":"Urban Education Explorer","text":"<p>Live Demo: https://public-data-graph-rag.netlify.app/ 2025-01-10</p>"},{"location":"portfolio/projects/project-1/#the-challenge","title":"The Challenge","text":"<p>The Urban Institute provides a massive repository of educational data, yet its complexity often hinders accessibility.</p> <ul> <li> <p>Extensive Data</p> <p>3,000+ variables and 40+ endpoints from 25+ sources (IPEDS, College Scorecard).</p> </li> <li> <p>Comprehensive Coverage</p> <p>Admissions, student outcomes, faculty, finance, safety, tax records, zip code, SES, and family income.</p> </li> </ul> <p>The challenge is that it's hard for people to know how to find what they are looking for. Navigating thousands of obscure variable codes (e.g., <code>cc_basic_2021</code>) requires extensive manual lookup, making it difficult to extract actionable insights.</p>"},{"location":"portfolio/projects/project-1/#our-approach","title":"Our Approach","text":"GraphRAG for Data Codes <p>Unlike standard AI chatbots that might hallucinate variable names, our system uses semantic vector search to map natural language queries directly to the authoritative variable codes in the database. When a user asks a question, the system retrieves the most relevant \"codes\" from the dataset documentation.</p> Knowledge Graph Context <p>These codes are not just retrieved as text; they are understood as nodes in a knowledge graph. The system identifies relationships\u2014such as which directory a variable belongs to (e.g., IPEDS), its format (percentage, currency), and its entity type. This allows the AI to construct accurate, data-grounded answers.</p> Interactive Visualization <p>Beyond chat, we integrated a full feature suite including interactive Leaflet maps for geospatial analysis and rich data tables. Users can filter institutions by specific criteria and see the results instantly on a map, or switch to the \"Applicants\" view to analyze student demographics and application flows.</p>"},{"location":"portfolio/projects/project-1/#results-impact","title":"Results &amp; Impact","text":"<ul> <li>Democratized Data Access: Transformed a technical API documentation site into a conversational interface, allowing non-technical users to query the database using plain English.</li> <li>Accurate Code Retrieval: The GraphRAG system successfully disambiguates between thousands of similar variable names, ensuring that the analysis is based on the correct underlying data fields.</li> <li>Unified Analysis Workflow: users can move seamlessly from asking high-level questions in the Chat interface to deep-diving into specific schools in the Explorer view, with all complex data codes handled automatically in the background.</li> </ul>"},{"location":"portfolio/projects/project-1/#example-use","title":"Example Use","text":"<p>Live Demo: https://public-data-graph-rag.netlify.app/</p> <ul> <li> <p>Calculated Geomaps</p> <p></p> <p>Fig 1: The Colleges view allows users to explore institutions geographically. It visualizes and displays school data across different education databases from IPEDS.</p> </li> <li> <p>Contextual Analysis</p> <p></p> <p>Fig 2: Deep dive into applicant demographics and admission probability analysis. Provides relative performance based on national aggregated data sets.</p> </li> <li> <p>Generative Chat</p> <p></p> <p>Fig 3: The Chat interface uses GraphRAG to map natural language queries to understand relevant data available in the Urban Education data sets.</p> </li> </ul>"},{"location":"portfolio/projects/project-1/#system-architecture","title":"System Architecture","text":"<p>Fig 4: The custom GraphRAG architecture combines serverless AI calls with client-side vector search for low-latency code retrieval.</p>"},{"location":"portfolio/projects/project-1/#tech-stack","title":"Tech Stack","text":"<ul> <li>Frontend: React, TypeScript, Tailwind CSS</li> <li>AI &amp; Logic: Google Gemini (via Google Generative AI SDK), GraphRAG Architecture (Custom implementation with Vector Search + Knowledge Graph logic)</li> <li>Mapping: Leaflet, React Leaflet</li> <li>Backend: Netlify Functions (Serverless)</li> <li>Data Processing: Client-side Vector Search (Cosine Similarity), Local JSON Data Stores</li> </ul>"},{"location":"portfolio/projects/project-2/","title":"Natural Language Processing for Quality Assurance for Constructed-Response Tests","text":"<p>Altus Assessments, 2022 2022-08-28</p>"},{"location":"portfolio/projects/project-2/#overview","title":"Overview","text":"<p>Developed novel architectures for automating the audit and validation of high-stakes textual assessments using Natural Language Processing (NLP). This innovation addressed the challenge of maintaining quality and fairness in large-scale constructed-response tests.</p>"},{"location":"portfolio/projects/project-2/#key-technologies","title":"Key Technologies","text":"<ul> <li>Natural Language Processing (NLP)</li> <li>Machine Learning</li> <li>Automated Auditing Algorithms</li> </ul>"},{"location":"portfolio/projects/project-2/#impact","title":"Impact","text":"<ul> <li>Improved the validity and reliability of high-stakes assessments.</li> <li>Reduced the manual workload for quality assurance.</li> <li>Enhanced fairness and equity in testing processes.</li> </ul>"},{"location":"portfolio/projects/project-3/","title":"Ability-based balancing for Exercise Video Games","text":"<p>Bloorview Research Institute, 2016 2016-06-01</p>"},{"location":"portfolio/projects/project-3/#overview","title":"Overview","text":"<p>Designed dynamic, ability-based balancing algorithms that utilize real-time biofeedback to tailor rehabilitation experiences in exercise video games (\"exergames\"). This technology ensures that therapeutic games remain challenging yet accessible for individuals with varying physical abilities, maximizing engagement and recovery outcomes.</p>"},{"location":"portfolio/projects/project-3/#key-technologies","title":"Key Technologies","text":"<ul> <li>Real-time Biofeedback</li> <li>Game Design &amp; Algorithms</li> <li>Biomedical Engineering</li> </ul>"},{"location":"portfolio/projects/project-3/#impact","title":"Impact","text":"<ul> <li>Drove measurable improvements in physical recovery outcomes.</li> <li>Personalized rehabilitation for patients.</li> <li>bridged Computer Science, Motor Control, and Biomedical Engineering.</li> </ul>"},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/","title":"Urban Education Explorer","text":""},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/#urban-education-explorer_1","title":"Urban Education Explorer","text":"<p>Live Demo: https://public-data-graph-rag.netlify.app/</p>"},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/#the-challenge","title":"The Challenge","text":"<p>The Urban Institute provides a massive repository of educational data, yet its complexity often hinders accessibility. *   Extensive Data: 3,000+ variables and 40+ endpoints from 25+ sources (IPEDS, College Scorecard). *   Comprehensive Coverage: Admissions, student outcomes, faculty, finance, safety, tax records, zip code, SES, and family income.</p> <p>The challenge is that it's hard for people to know how to find what they are looking for. Navigating thousands of obscure variable codes (e.g., <code>cc_basic_2021</code>) requires extensive manual lookup, making it difficult to extract actionable insights.</p>"},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/#our-approach","title":"Our Approach","text":"<p>We built the Urban Education Explorer, an intelligent web application designed to bridge the gap between complex data repositories and human curiosity. The core of this solution is a specialized GraphRAG (Graph Retrieval-Augmented Generation) architecture designed specifically to \"understand\" the Urban Education Data Set.</p> <ul> <li>GraphRAG for Data Codes: Unlike standard AI chatbots that might hallucinate variable names, our system uses semantic vector search to map natural language queries directly to the authoritative variable codes in the database. When a user asks a question, the system retrieves the most relevant \"codes\" from the dataset documentation.</li> <li>Knowledge Graph Context: These codes are not just retrieved as text; they are understood as nodes in a knowledge graph. The system identifies relationships\u2014such as which directory a variable belongs to (e.g., IPEDS), its format (percentage, currency), and its entity type. This allows the AI to construct accurate, data-grounded answers.</li> <li>Interactive Visualization: Beyond chat, we integrated a full feature suite including interactive Leaflet maps for geospatial analysis and rich data tables. Users can filter institutions by specific criteria and see the results instantly on a map, or switch to the \"Applicants\" view to analyze student demographics and application flows.</li> </ul>"},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/#results-impact","title":"Results &amp; Impact","text":"<ul> <li>Democratized Data Access: Transformed a technical API documentation site into a conversational interface, allowing non-technical users to query the database using plain English.</li> <li>Accurate Code Retrieval: The GraphRAG system successfully disambiguates between thousands of similar variable names, ensuring that the analysis is based on the correct underlying data fields.</li> <li>Unified Analysis Workflow: users can move seamlessly from asking high-level questions in the Chat interface to deep-diving into specific schools in the Explorer view, with all complex data codes handled automatically in the background.</li> </ul>"},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/#visual-assets","title":"Visual Assets","text":"<p> Fig 1: The Colleges view allows users to explore institutions geographically. It visualizes and displays school data across different education databases from IPEDS.</p> <p> Fig 2: Deep dive into applicant demographics and admission probability analysis. Provides relative performance based on national aggregated data sets.</p> <p> Fig 3: The Chat interface uses GraphRAG to map natural language queries to understand relevant data available in the Urban Education data sets.</p>"},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/#system-architecture","title":"System Architecture","text":"<pre><code>graph TD\n    User[User Query] --&gt; Client[React Client]\n\n    subgraph \"GraphRAG Flow\"\n    Client --&gt;|1. Request Embedding| NetlifyEmbed[\"Netlify Function\\n(Embedding)\"]\n    NetlifyEmbed &lt;--&gt;|Google GenAI SDK| GeminiEmbed[\"Gemini\\nEmbedding Model\"]\n\n    Client --&gt;|2. Vector Search| LocalSearch[\"Local Vector Store\\n(Cosine Similarity)\"]\n    LocalSearch &lt;--&gt;|Read| NodeData[(\"node_embeddings.json\")]\n\n    Client --&gt;|3. Context Expansion| KG[\"Knowledge Graph\\nLogic\"]\n    KG &lt;--&gt;|Lookups| StaticData[(\"node_names.txt\")]\n\n    Client --&gt;|4. Generate Response| NetlifyGen[\"Netlify Function\\n(Chat)\"]\n    NetlifyGen &lt;--&gt;|Prompt + Context| GeminiGen[\"Gemini\\nFlash Lite\"]\n    end\n\n    NetlifyGen --&gt;|Final Response| Client</code></pre> <p>Fig 4: The custom GraphRAG architecture combines serverless AI calls with client-side vector search for low-latency code retrieval.</p>"},{"location":"portfolio/projects/portfolio-project-1/urban_education_explorer_portfolio_item/#tech-stack","title":"Tech Stack","text":"<ul> <li>Frontend: React, TypeScript, Tailwind CSS</li> <li>AI &amp; Logic: Google Gemini (via Google Generative AI SDK), GraphRAG Architecture (Custom implementation with Vector Search + Knowledge Graph logic)</li> <li>Mapping: Leaflet, React Leaflet</li> <li>Backend: Netlify Functions (Serverless)</li> <li>Data Processing: Client-side Vector Search (Cosine Similarity), Local JSON Data Stores</li> </ul>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/category/data-engineering/","title":"Data Engineering","text":""},{"location":"blog/category/ai-context/","title":"AI Context","text":""},{"location":"blog/category/urban-education-explorer/","title":"urban-education-explorer","text":""},{"location":"blog/category/ai-research/","title":"AI Research","text":""},{"location":"blog/category/edtech/","title":"EdTech","text":""}]}